{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import pandas as pd\n",
    "import subprocess\n",
    "from phonlab.utils import dir2df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phonlab tasks\n",
    "\n",
    "This notebook contains short articles on common data analysis tasks you might need for your work in the Phonlab.\n",
    "\n",
    "1. [A sample post-processing workflow](#sample_workflow)\n",
    "    1. [Mirror a directory structure](#mirror_directory)\n",
    "    1. [Find rows in a source DataFrame that do not have a match in a second DataFrame](#find_non_matched_rows)\n",
    "    1. [Perform a task for every row in a DataFrame](#task_per_df_row)\n",
    "    1. [Summary of the post-processing workflow](#sample_workflow_summary)\n",
    "1. [Collecting results for analysis](#collecting_results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"sample_workflow\"></a>A sample post-processing workflow\n",
    "\n",
    "A typical step in your data analysis pipeline is to do post-processing on a dataset. This dataset may contain a number of audio recordings, for instance, and you wish to perform formant analysis on each of them, then collect the results into a DataFrame for statistical analysis. This section covers the steps involved in post-processing the files, the formant analysis part.\n",
    "\n",
    "Sometimes you have a complete dataset already in hand and need to do post-processing only once. Other times you may have a dataset that grows incrementally, perhaps as new subjects are included. The steps you will see are modular and can be repeated (or skipped) so that incremental additions can be updated with reasonable efficiency. For very large datasets a more sophisticated workflow might be necessary.\n",
    "\n",
    "In our example we will perform formant analysis on a set of .wav files found in the dataset. The directory that contains the .wav files is the source directory `srcdir`. We will write the formant measurements to a separate directory with the same internal structure as `srcdir`, and we will refer to this directory as the `cachedir` because it holds cached results obtained from the source data files.\n",
    "\n",
    "The steps in the workflow are:\n",
    "\n",
    "1. [Load filenames in `srcdir` and add analysis parameters](#load_filenames_and_parameters)\n",
    "1. [Find filenames in `srcdir` that require post-processing](#find_non_matched_rows)\n",
    "1. [Mirror the directory structure](#mirror_directory) (copy the internal structure of `srcdir` to `cachedir` in preparation for writing formant measurements)\n",
    "1. [Perform a task for every row in a DataFrame](#task_per_df_row) (do formant analysis)\n",
    "\n",
    "These steps are summarized in the tl;dr section [Summary of the post-processing workflow](#sample_workflow_summary)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load filenames in `srcdir` and add analysis parameters\n",
    "\n",
    "In this section we load the filenames of the .wav files in `srcdir` and add analysis parameters to be used by `ifcformant`. First we define the locations of the source and cache directories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "srcdir = '../resource/postproc/orig_data'\n",
    "cachedir = '../resource/postproc/cache'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***WARNING:*** `cachedir` must not be contained anywhere under `srcdir`! The mirroring technique described in this article is very simple to implement but may produce unexpected results if `cachedir` is part of `srcdir`. It is okay if `cachedir` is a sibling of `srcdir`, as they are defined above, but avoid this:\n",
    "\n",
    "```python\n",
    "# This is not okay!\n",
    "srcdir = '../resource/postproc/orig_data'\n",
    "cachedir = '../resource/postproc/orig_data/cache' # cachedir inside srcdir!\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load filenames from `srcdir`\n",
    "\n",
    "The [`dir2df()` function](Retrieving%20filenames%20in%20a%20directory%20tree%20with%20%60dir2df%28%29%60.ipynb) makes it easy to load the set of filenames of the .wav files in `srcdir`. We use a [named capture](Retrieving%20filenames%20in%20a%20directory%20tree%20with%20%60dir2df%28%29%60.ipynb#adding_variables_named_capture) to also extract the subject identifier and [`addcols`](Retrieving%20filenames%20in%20a%20directory%20tree%20with%20%60dir2df%28%29%60.ipynb#adding_variables_addcols) to add the file's barename. The barename will make it easy to match .ifc files, as you will see shortly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "srcdf = dir2df(\n",
    "    srcdir,\n",
    "    dirpat='^(?P<subject>subj\\d+)',\n",
    "    fnpat='\\.wav$',\n",
    "    addcols=['barename']\n",
    ")\n",
    "srcdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add analysis parameters\n",
    "\n",
    "The `ifcformant` command requires two parameters, 1) the name of the input .wav file; and 2) the speaker type ('female', 'male', or 'child'). The first parameter is already available in `srcdf`, and we need to add the second.\n",
    "\n",
    "If you code speaker type into your filenames or directory structure, you can [extract speaker type](Retrieving%20filenames%20in%20a%20directory%20tree%20with%20%60dir2df%28%29%60.ipynb#adding_variables) when you call `dir2df()` and skip the rest of this section.\n",
    "\n",
    "If you do not encode speaker type in your filename, you may load speaker type from an external file using one of [Pandas' Input/Output functions](https://pandas.pydata.org/pandas-docs/stable/api.html#input-output), most likely either [`read_csv()`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html#pandas.read_csv) or [`read_excel`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_excel.html#pandas.read_excel). We'll use `read_csv()` to load metadata from a metadata file in `srcdir`. The 'speaker_metadata.csv' file contains two comma-separated columns labelled 'subject' and 'sex'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "md = pd.read_csv(os.path.join(srcdir, 'speaker_metadata.csv'))\n",
    "md"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A left merge adds speaker type to `srcdf`. In order for this merge to work properly, the values of one of the columns in `srcdf` must match the values of one of the columns in `md`. Here the columns named 'subject' match and we merge `on` those columns. (If the column names don't match you can use `left_on` and `right_on` instead of `on`.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "srcdf = srcdf.merge(md, on='subject', how='left')\n",
    "srcdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The left merge ensures that all of the rows in `srcdf` are in the merge output. If you find a NaN value in the output, it means that `md` is missing a subject and you should update your metadata document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a name=\"find_non_matched_rows\"></a>Find filenames in `srcdir` that require post-processing\n",
    "\n",
    "The next step in the workflow is to find existing `ifcformant` output files in `cachedir` and use that result to find the rows in `srcdf` that do ***not*** have a corresponding output file. These are the files that require post-processing.\n",
    "\n",
    "In general, if you have a source DataFrame and want to check whether a second DataFrame has a corresponding row, you can use a left merge with the source on the lefthand side. A left merge returns all of the merge keys from the lefthand DataFrame regardless of whether a matching key is found on the right. When there is no match, the columns from the right DataFrame are filled with NaN.\n",
    "\n",
    "For this example the left DataFrame will be `srcdf`. The right DataFrame will be created from `cachedir`. We load the .ifc files in `cachedir` into `cachedf`. As you can see, the formant measurements for the first two acquisitions for subj1 have already been created and cached in .ifc files that correspond to the .wav files in the relative path 'subj1/trial1'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cachedf = dir2df(cachedir, fnpat='\\.ifc$', addcols=['barename'])\n",
    "cachedf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our goal is to find each '.wav' file in `srcdf` that does not have a corresponding '.ifc' file in `cachedf`, and we do this in part by matching the barename values--each 'acq1.wav' should match an 'acq2.ifc'. It is not enough to match the barename values in the two DataFrames, however. This is because the barenames are not unique in the global dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "srcdf[srcdf.barename == 'acq1']   # barename 'acq1' appears four times"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To fully identify each .wav file in `srcdf` it is necessary to include 'relpath' as well as 'barename'. The combination of these two column uniquely identifies each row in `srcdf`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another left merge, with `srcdf` as the left DataFrame and `cachedf` as the right, matches .wav input files with cached .ifc files, using `relpath` and `barename` as the complex merge key. Since the `fname` column is in both `srcdf` and `cachedf` we provide suffixes to append to those column names in the merged DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mrgdf = srcdf.merge(\n",
    "    cachedf,\n",
    "    on=['relpath', 'barename'],\n",
    "    how='left',\n",
    "    suffixes=['_wav', '_ifc']\n",
    ")\n",
    "mrgdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select the rows of `mrgdf` that have a value of NaN in the `fname_ifc` column. These represent the .wav files that do not have cached .ifc measurements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "noifcdf = mrgdf[mrgdf.fname_ifc.isnull()]\n",
    "noifcdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a name=\"mirror_directory\"></a>Mirror the directory structure\n",
    "\n",
    "We have already seen that the directory structure of `srcdir` organizes files into trial subdirectories nested inside subject directories. We must create the same directory structure in `cachedir` before running `ifcformant`. The `ifcformant` command will fail if a writeable output directory does not exist for the .ifc file.\n",
    "\n",
    "The directory structure is provided by the `relpath` column, and the unique values of it in the `noifcdf` provide all of the directory names that must exist prior to running `ifcformant`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "unique_relpath = noifcdf.relpath.unique()\n",
    "unique_relpath"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simple way to copy the structure is to loop over the unique set of relative paths and create them in `cachedir` using [os.makedirs()](https://docs.python.org/3/library/os.html#os.makedirs).\n",
    "\n",
    "The following cell will copy the required directory structure to `cachedir` and print a success message if it succeeds. If there are any problems in creating a directory an error will be raised instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for destdir in unique_relpath:\n",
    "    os.makedirs(\n",
    "        os.path.join(cachedir, destdir),  # e.g. ../resource/postproc/cache/subj1/trial1\n",
    "        exist_ok=True\n",
    "    )\n",
    "sys.stderr.write('Directory mirroring succeeded.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `os.makedirs()` function automatically creates parent directories where necessary. For instance, the first relative path in the example above is `subj1/trial2`, and the first call to `os.makedirs()` is a request to create the directory `../resource/postproc/cache/subj1/trial1`. If `../resource/postproc/cache/subj1` does not exist already, then that directory will be created first.\n",
    "\n",
    "The `exist_ok=True` means that `os.makedirs()` will not raise an error if the target directory already exists. This behavior is convenient for mirroring a `srcdir` incrementally. If you add `subj3/trial1` and `subj3/trial2` directories after running the above cell, then you can simply append them to `unique_relpath` and re-rerun the loop without raising an error for the existing directories under `subj1` and `subj2`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a name=\"task_per_df_row\"></a>Perform a task for every row in a DataFrame\n",
    "\n",
    "The `noifcdf` DataFrame contains the names of .wav files that require formant analysis and the speaker analysis parameters. In this section we'll construct a function for doing formant analysis that uses the rows of `noifcdf` as its inputs. As a reminder, `noifcdf` contains:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "noifcdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Iterate over rows with `itertuples()`\n",
    "\n",
    "The [`itertuples()` method](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.itertuples.html#pandas.DataFrame.itertuples) iterates over the rows of a DataFrame and returns each row as a [namedtuple](https://docs.python.org/3/library/collections.html#collections.namedtuple).\n",
    "\n",
    "***Aside:*** You might come across a similarly named method `iterrows()`, and it is recommended that you avoid it. The `iterrows()` method is less convenient than `itertuples()` because 1) it doesn't provide access to column values by name; and 2) it is slower to execute than `itertuples()`.\n",
    "\n",
    "Here is a simple example that prints the values that `itertuples()` returns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for row in noifcdf.itertuples():\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the value of the row index is added as the first value and is named 'Index'. The DataFrame column labels are the other attributes and are easily accessed by name with attribute '.' notation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for row in noifcdf.itertuples():\n",
    "    print(row.fname_wav)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It makes sense to put your task in a named function when the task you wish to perform is more complicated than a simple print statement. Doing so helps make your code easier to debug, and you can re-use the function in multiple places.\n",
    "\n",
    "The `my_print` function uses `os.path.join` to construct a filepath from the 'relpath' and 'fname' attributes of a row and prints the result. The `for` loop calls `my_print` on each row in turn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Function definition.\n",
    "def my_print(rowtuple):\n",
    "    '''Print values from a DataFrame row provided as a namedtuple.'''\n",
    "    print(rowtuple.subject)\n",
    "    print(os.path.join(rowtuple.relpath, rowtuple.barename + '.ifc'))\n",
    "    print(os.path.join(rowtuple.relpath, rowtuple.fname_wav))\n",
    "    print('******************')\n",
    "\n",
    "# Loop over rows and call the `my_print` function.\n",
    "for row in noifcdf.itertuples():\n",
    "    my_print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The `ifcformant` function\n",
    "\n",
    "Now that we know the mechanics of calling a named function for every row in the DataFrame, let's construct a function that runs the `ifcformant` command using the parameters provided by a namedtuple.\n",
    "\n",
    "If we were working at the command line, a representative example of calling `ifcformant` is:\n",
    "\n",
    "```\n",
    "ifcformant --speaker female --print-header --output myfile.ifc myfile.wav\n",
    "```\n",
    "\n",
    "The arguments to `ifcformant` include the speaker type, the name of the output file that will contain the formant measurements, and the input '.wav' file. The `--print-header` argument is used to print column labels as the first row of the output file.\n",
    "\n",
    "The `do_ifcformant` function shown below constructs an array of arguments from an input namedtuple and output directory, then uses the [`subprocess` module](https://docs.python.org/3/library/subprocess.html) to execute `ifcformant`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def do_ifcformant(rt, srcdir, outdir, errors='raise'):\n",
    "    '''Perform formant analysis with the ifcformant command.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    \n",
    "    rt : namedtuple that contains formant analysis parameters\n",
    "         in fields:\n",
    "         'relpath' (relative path to audio file),\n",
    "         'fname' (name of .wav file),\n",
    "         'barename' (name of .wav file without extension)\n",
    "         'speaker' (ifcformant speaker type, one of 'female',\n",
    "             'male', 'child')\n",
    "             \n",
    "    srcdir : str\n",
    "        Base pathname to input .wav file. The path to the input file\n",
    "        will be: srcdir/rt.relpath/rt.fname_wav.\n",
    "             \n",
    "    outdir : str\n",
    "        Base pathname to ifcformant output. The output file will\n",
    "        be written to: outdir/rt.relpath/rt.barename + '.ifc'.\n",
    "             \n",
    "    errors : str (default 'raise')\n",
    "        How to handle errors if `check_call()` fails. If\n",
    "        'ignore', print debug statement to STDERR and return the\n",
    "        ifcformant return code; if 'raise' immediately reraise\n",
    "        the CalledProcessError.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    \n",
    "    The `ifcformant` return code is returned by this function,\n",
    "    0 for success or non-zero for errors.\n",
    "    '''\n",
    "    ifcargs = [\n",
    "        'ifcformant',\n",
    "        '--speaker', rt.sex,\n",
    "        '--print-header',\n",
    "        '--output', os.path.join(\n",
    "            outdir, rt.relpath, rt.barename + '.ifc'\n",
    "        ),\n",
    "        os.path.join(srcdir, rt.relpath, rt.fname_wav)\n",
    "    ]\n",
    "    try:\n",
    "        subprocess.check_call(ifcargs)\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        if errors == 'ignore':\n",
    "            msg = 'Caught error while invoking ifcformant:\\n{:}'.format(e)\n",
    "            sys.stderr.write(msg)\n",
    "            return e.returncode\n",
    "        else:\n",
    "            raise e\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's always best to include a docstring at the top of your named functions. This helps you document your workflow and to possibly re-use the function in another project. Execute the following cell to see the documentation for the new function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "do_ifcformant?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once your function is created and debugged, use `itertuples()` to run the function on every row of your DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for row in noifcdf.itertuples():\n",
    "    do_ifcformant(row, srcdir=srcdir, outdir=cachedir)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `dir2df()` to list the .ifc files in `cachedir`. All but two should show recent mtime values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dir2df(cachedir, fnpat='\\.ifc$', addcols=['mtime'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a name=\"sample_workflow_summary\"></a>Summary of the post-processing workflow\n",
    "\n",
    "This section contains a summary of the post-processing workflow with minimal explanation. Each step is in a separate cell to make it easy to execute each separately, in modular fashion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define source and cache directories.\n",
    "srcdir = '../resource/postproc/orig_data'\n",
    "cachedir = '../resource/postproc/cache'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load .wav filenames from srcdf.\n",
    "srcdf = dir2df(\n",
    "    srcdir,\n",
    "    dirpat='^(?P<subject>subj\\d+)',\n",
    "    fnpat='\\.wav$',\n",
    "    addcols=['barename']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load speaker metadata and merge with srcdf.\n",
    "md = pd.read_csv(os.path.join(srcdir, 'speaker_metadata.csv'))\n",
    "srcdf = srcdf.merge(md, on='subject', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load cached .ifc filenames and merge with srcdf.\n",
    "cachedf = dir2df(cachedir, fnpat='\\.ifc$', addcols=['barename'])\n",
    "mrgdf = srcdf.merge(\n",
    "    cachedf,\n",
    "    on=['relpath', 'barename'],\n",
    "    how='left',\n",
    "    suffixes=['_wav', '_ifc']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Select .wav files that do not have a corresponding cached .ifc file.\n",
    "noifcdf = mrgdf[mrgdf.fname_ifc.isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Mirror srcdir directory structure in cachedir.\n",
    "unique_relpath = noifcdf.relpath.unique()\n",
    "for destdir in unique_relpath:\n",
    "    os.makedirs(\n",
    "        os.path.join(cachedir, destdir),  # e.g. ../resource/postproc/cache/subj1/trial1\n",
    "        exist_ok=True\n",
    "    )\n",
    "sys.stderr.write('Directory mirroring succeeded.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Run ifcformant on .wav files and output to cachedir.\n",
    "# NOTE: do_ifcformant() function must already be defined.\n",
    "for row in noifcdf.itertuples():\n",
    "    do_ifcformant(row, outdir=cachedir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
